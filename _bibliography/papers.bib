---
---
@article{pokeflex,
  author    = {Obrist, Jan and Zamora, Miguel and Zheng, Hehui and Hinchet, Ronan and Ozdemir, Firat and Zarate, Juan and Katzschmann, Robert K. and Coros, Stelian},
  title     = {PokeFlex: a Real-World Dataset of Deformable Objects for Robotics},
  year      = {2024},
  journal   = {Preprint},
  preview={dice_poke_v2.gif},
  website = {https://pokeflex-dataset.github.io/},  
  selected={true},
  abstract={Data-driven methods have shown great potential in solving challenging manipulation tasks, however, their application in the domain of deformable objects has been constrained, in part, by the lack of data. To address this, we propose PokeFlex, a dataset featuring real-world paired and annotated multimodal data that includes 3D textured meshes, point clouds, RGB images, and depth maps. Such data can be leveraged for several downstream tasks such as online 3D mesh reconstruction, and it can potentially enable underexplored applications such as the real-world deployment of traditional control methods based on mesh simulations. To deal with the challenges posed by real-world 3D mesh reconstruction, we leverage a professional volumetric capture system that allows complete 360° reconstruction. PokeFlex consists of 18 deformable objects with varying stiffness and shapes. Deformations are generated by dropping objects onto a flat surface or by poking the objects with a robot arm. Interaction forces and torques are also reported for the latter case. Using different data modalities, we demonstrated a use case for the PokeFlex dataset in online 3D mesh reconstruction. We refer the reader to our website for demos and examples of our dataset.},
}

@article{tapas,
  author    = {Zamora, Miguel and Hartmann, Valentin N. and Coros, Stelian},
  title     = {TAPAS: A Dataset for Task Assignment and Planning for Multi Agent Systems},
  year      = {2024},
  journal   = {Workshop on Data Generation for Robotics at Robotics, Science and Systems '24},
  preview={random_4_objs.gif},
  website = {https://tapas-dataset.github.io/},
  selected={true},
  abstract={Obtaining real world data for robotics tasks is harder than for other modalities such as vision and text. The data that is currently available for robot learning is mostly set in static scenes, and deals with a single robot only. Dealing with multiple robots comes with additional difficulties compared to single robot settings: the motion planning for multiple agents needs to take into account the movement of the other robots, and task planning needs to consider to which robot a task is assigned to, in addition to when a task should be done. In this work, we present TAPAS, a simulated dataset containing task and motion plans for multiple robots acting asynchronously in the same workspace and modifying the same environment. We consider prehensile manipulation in this dataset, and focus on various pick and place tasks. We demonstrate that training using this data for predicting makespan of a task sequence enables speeding up finding low makespan sequences by ranking sequences before computing the full motion plan.},
}


@article{kang2023rl+,
  title={RL+ Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion},
  author={Kang, Dongho and Cheng, Jin and Zamora, Miguel and Zargarbashi, Fatemeh and Coros, Stelian},
  journal={IEEE Robotics and Automation Letters (RA-L)},
  year={2023},
  arxiv={2305.17842},
  html={https://ieeexplore.ieee.org/document/10225268},
  video={https://youtu.be/gXDP87yVq4o},
  preview={kang2023rl.png},
  abstract={This letter presents a versatile control method for dynamic and robust legged locomotion that integrates model-based optimal control with reinforcement learning (RL). Our approach involves training an RL policy to imitate reference motions generated on-demand through solving a finite-horizon optimal control problem. This integration enables the policy to leverage human expertise in generating motions to imitate while also allowing it to generalize to more complex scenarios that require a more complex dynamics model. Our method successfully learns control policies capable of generating diverse quadrupedal gait patterns and maintaining stability against unexpected external perturbations in both simulation and hardware experiments. Furthermore, we demonstrate the adaptability of our method to more complex locomotion tasks on uneven terrain without the need for excessive reward shaping or hyperparameter tuning.},
  selected={false},
  website={https://donghok.me/rl-plus-model-based-control/}
}

@InProceedings{hartmann2024deep,
  author={Hartmann, Adrian and Kang, Dongho and Zargarbashi, Fatemeh and Zamora, Miguel and Coros, Stelian},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Deep Compliant Control for Legged Robots}, 
  year={2024},
  pages={11421-11427},
  keywords={Training;Legged locomotion;Uncertainty;Tracking;Perturbation methods;Process control;Energy efficiency},
  doi={10.1109/ICRA57147.2024.10611209},
  selected={false},
  bibtex_show={false},
  preview={hartmann2024deep.png},
  video = {https://youtu.be/vgeEoK78doA},
  html = {https://ieeexplore.ieee.org/document/10611209},
  abstract = {Control policies trained using deep reinforcement learning often generate stiff, high-frequency motions in response to unexpected disturbances. To promote more natural and compliant balance recovery strategies, we propose a simple modification to the typical reinforcement learning training process. Our key insight is that stiff responses to perturbations are due to an agent’s incentive to maximize task rewards at all times, even as perturbations are being applied. As an alternative, we introduce an explicit recovery stage where tracking rewards are given irrespective of the motions generated by the control policy. This allows agents a chance to gradually recover from disturbances before attempting to carry out their main tasks. Through an in-depth analysis, we highlight both the compliant nature of the resulting control policies, as well as the benefits that compliance brings to legged locomotion. In our simulation and hardware experiments, the compliant policy achieves more robust, energy-efficient, and safe interactions with the environment.},
}

@article{wenbo2,     
  author = {Wenbo Wang and Gen Li and Miguel Zamora and Stelian Coros},
  journal={2024 IEEE International Conference on Robotics and Automation (ICRA)},  
  title = {TRTM: Template-based Reconstruction and Target-oriented Manipulation of Crumpled Cloths},
  preview={TRTM.png},  
  year = {2024},
  arxiv={2308.04670},  
  website={https://wenbwa.github.io/TRTM/},
  video = {https://www.youtube.com/watch?v=C0rrODnZKsc},
  html = {https://ieeexplore.ieee.org/document/10609868},
  selected={false},
  abstract={Precise reconstruction and manipulation of the crumpled cloths is challenging due to the high dimensionality of cloth models, as well as the limited observation at self-occluded regions. We leverage the recent progress in the field of single-view reconstruction to template-based reconstruct the crumpled cloths from their top-view depth observations only, with our proposed sim-real registration protocols. In contrast to previous implicit cloth representations, our reconstruction mesh explicitly describes the positions and visibilities of the entire cloth mesh vertices, enabling more efficient dual-arm and single-arm target-oriented manipulations. Experiments demonstrate that our TRTM system can be applied to daily cloths that have similar topologies as our template mesh, but with different shapes, sizes, patterns, and physical properties. Videos, datasets, pre-trained models, and code can be downloaded from our project website: https://wenbwa.github.io/TRTM/.},
}

@article{zhaoting,
      title={Embracing Safe Contacts with Contact-aware Planning and Control}, 
      author={Zhaoting Li and Miguel Zamora and Hehui Zheng and Stelian Coros},
      journal={RSS 2023. Workshop: Experiment-oriented Locomotion and Manipulation Research},  
      preview={contact.png},
      year={2023},
      arxiv={2308.04323},
      video={https://www.youtube.com/watch?v=2WeYytauhNg},
      abstract={Unlike human beings that can employ the entire surface of their limbs as a means to establish contact with their environment, robots are typically programmed to interact with their environments via their end-effectors, in a collision-free fashion, to avoid damaging their environment. In a departure from such a traditional approach, this work presents a contact-aware controller for reference tracking that maintains interaction forces on the surface of the robot below a safety threshold in the presence of both rigid and soft contacts. Furthermore, we leveraged the proposed controller to extend the BiTRRT sample-based planning method to be contact-aware, using a simplified contact model. The effectiveness of our framework is demonstrated in hardware experiments using a Franka robot in a setup inspired by the Amazon stowing task.},
}

@INPROCEEDINGS{Bhavya,
  author={Sukhija, Bhavya and Köhler, Nathanael and Miguel Zamora and Zimmermann, Simon and Curi, Sebastian and Krause, Andreas and Coros, Stelian},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Gradient-Based Trajectory Optimization With Learned Dynamics}, 
  preview={icra_gradient_based.png},  
  year={2023},
  volume={},
  number={},
  pages={1011-1018},
  doi={10.1109/ICRA48891.2023.10161574},  
  selected={false},
  html = 	 {https://ieeexplore.ieee.org/document/10161574},
  video = {https://www.youtube.com/watch?v=yj-iFPIAW2o},  
  arxiv = {2204.04558},
  abstract={Trajectory optimization methods have achieved an exceptional level of performance on real-world robots in recent years. These methods heavily rely on accurate analytical models of the dynamics, yet some aspects of the physical world can only be captured to a limited extent. An alternative approach is to leverage machine learning techniques to learn a differentiable dynamics model of the system from data. In this work, we use trajectory optimization and model learning for performing highly dynamic and complex tasks with robotic systems in absence of accurate analytical models of the dynamics. We show that a neural network can model highly nonlinear behaviors accurately for large time horizons, from data collected in only 25 minutes of interactions on two distinct robots: (i) the Boston Dynamics Spot and an (ii) RC car. Furthermore, we use the gradients of the neural network to perform gradient-based trajectory optimization. In our hardware experiments, we demonstrate that our learned model can represent complex dynamics for both the Spot and Radio-controlled (RC) car, and gives good performance in combination with trajectory optimization methods.},
}  

@ARTICLE{CURRY,
  author={Zamora, Miguel and Poranne, Roi and Coros, Stelian},
  journal={IEEE Robotics and Automation Letters}, 
  title={Learning Solution Manifolds for Control Problems via Energy Minimization}, 
  preview={zamora2022learning.png},  
  year={2022},
  volume={7},
  number={3},
  pages={7912-7919},
  doi={10.1109/LRA.2022.3186046},
  selected={true},  
  html = 	 {https://ieeexplore.ieee.org/document/9806174},
  video = {https://www.youtube.com/watch?v=0JXD0h5pOBk},
  abstract={A variety of control tasks such as inverse kinematics (IK), trajectory optimization (TO), and model predictive control (MPC) are commonly formulated as energy minimization problems. Numerical solutions to such problems are well-established. However, these are often too slow to be used directly in real-time applications. The alternative is to learn solution manifolds for control problems in an offline stage. Although this distillation process can be trivially formulated as a behavioral cloning (BC) problem, our experiments highlight a number of significant shortcomings arising due to incompatible local minima, interpolation artifacts, and insufficient coverage of the state space. In this paper, we propose an alternative to BC that is efficient and numerically robust. We formulate the learning of solution manifolds as a minimization of the energy terms of a control objective integrated over the space of problems of interest. We minimize this energy integral with a novel method that combines Monte Carlo-inspired adaptive sampling strategies with the derivatives used to solve individual instances of the control task. We evaluate the performance of our formulation on a series of robotic control problems of increasing complexity, and we highlight its benefits through comparisons against traditional methods such as behavioral cloning and Dataset aggregation (Dagger).},
}

@article{pods,
  title = 	 {PODS: Policy Optimization via Differentiable Simulation},
  author = { Miguel Zamora and Peychev, Momchil and Ha, Sehoon and Vechev, Martin and Coros, Stelian},
  journal={Proceedings of the 38th International Conference on Machine Learning, (ICML) },
  preview={pods.png},  
  year={2021},    
  html = 	 {https://proceedings.mlr.press/v139/mora21a.html},
  video = 	 {https://www.youtube.com/watch?v=Hz7rfhn-f44},
  selected={true},
  abstract={Current reinforcement learning (RL) methods use simulation models as simple black-box oracles. In this paper, with the goal of improving the performance exhibited by RL algorithms, we explore a systematic way of leveraging the additional information provided by an emerging class of differentiable simulators. Building on concepts established by Deterministic Policy Gradients (DPG) methods, the neural network policies learned with our approach represent deterministic actions. In a departure from standard methodologies, however, learning these policies does not hinge on approximations of the value function that must be learned concurrently in an actor-critic fashion. Instead, we exploit differentiable simulators to directly compute the analytic gradient of a policy’s value function with respect to the actions it outputs. This, in turn, allows us to efficiently perform locally optimal policy improvement iterations. Compared against other state-of-the-art RL methods, we show that with minimal hyper-parameter tuning our approach consistently leads to better asymptotic behavior across a set of payload manipulation tasks that demand a high degree of accuracy and precision.},
}

@article{zimmermann2020multileveloptimizaitonframeworkforsimultaneousgraspingandmotionplanning,
  title={A Multi-Level Optimization Framework for Simultaneous Grasping and Motion Planning}, 
  author={Zimmermann, Simon and Hakimifard, Ghazal and Miguel Zamora and Poranne, Roi and Coros, Stelian},
  journal={IEEE Robotics and Automation Letters},   
  preview={castleMaker.png},  
  year={2020},
  volume={5},
  number={2},
  pages={2966-2972},
  doi={10.1109/LRA.2020.2974684}, 
  selected={false},
  html={https://ieeexplore.ieee.org/document/9001184},
  video={https://www.youtube.com/watch?v=7GAAFKE2VL8},
  abstract={We present an optimization framework for grasp and motion planning in the context of robotic assembly. Typically, grasping locations are provided by higher level planners or as input parameters. In contrast, our mathematical model simultaneously optimizes motion trajectories, grasping locations, and other parameters such as the pose of an object during handover operations. The input to our framework consists of a set of objects placed in a known configuration, their target locations, and relative timing information describing when objects need to be picked up, optionally handed over, and dropped off. To allow robots to reason about the way in which grasping locations govern optimal motions, we formulate the problem using a multi-level optimization scheme: the top level optimizes grasping locations; the mid-layer level computes the configurations of the robot for pick, drop and handover states; and the bottom level computes optimal, collision-free motions. We leverage sensitivity analysis to compute derivatives analytically (how do grasping parameters affect IK solutions, and how these, in turn, affect motion trajectories etc.), and devise an efficient numerical solver to generate solutions to the resulting optimization problem. We demonstrate the efficacy of our approach on a variety of assembly and handover tasks performed by a dual-armed robot with parallel grippers.},
}
















